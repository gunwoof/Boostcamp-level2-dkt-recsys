import os
import random
import time
import sys
from datetime import datetime
from typing import Tuple

import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import LabelEncoder

# feature_engineering ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))) + '/Feature_Engineering')
#import sehoon.feat_eng_sehoon as sehoon

# logger ì¶”ê°€
from dkt.utils import get_logger, set_seeds, logging_conf, get_data_info
logger = get_logger(logging_conf)


class Preprocess:
    def __init__(self, args):
        self.args = args
        self.train_data = None
        self.test_data = None

    def get_train_data(self):
        return self.train_data

    def get_test_data(self):
        return self.test_data

    def __save_labels(self, encoder: LabelEncoder, name: str) -> None:
        le_path = os.path.join(self.args.asset_dir, name + "_classes.npy")
        np.save(le_path, encoder.classes_)

    # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”©
    def __preprocessing(self, df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:
        
        # TimestampëŠ” Feature Engineering í–ˆë‹¤ê³  ë³´ê³  ì‚­ì œ ì¡°ì¹˜
        df = df.drop('Timestamp', axis=1)
        
        X_columns = [column for column in df.columns if column not in self.args.tgt_col + self.args.user_col]

        # ì¹´í…Œê³ ë¦¬ feature ì§€ì • : cat_ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼
        default_cate_cols = ["assessmentItemID", "testId", "KnowledgeTag"]
        for col in X_columns:
            if col.startswith('cat_'):
                default_cate_cols.append(col)
        setattr(self.args, 'cat_cols', default_cate_cols)
        setattr(self.args, 'con_cols', [col for col in X_columns if col not in self.args.cat_cols])
        
        if not os.path.exists(self.args.asset_dir):
            os.makedirs(self.args.asset_dir)

        for col in self.args.cat_cols:
            le = LabelEncoder()
            if is_train:
                # For UNKNOWN class
                a = df[col].unique().tolist() + ["unknown"]
                le.fit(a)
                self.__save_labels(le, col)
            else:
                label_path = os.path.join(self.args.asset_dir, col + "_classes.npy") # ê°œìˆ˜ë¥¼ ë¹¼ë‘ 
                le.classes_ = np.load(label_path)

                df[col] = df[col].apply(
                    lambda x: x if str(x) in le.classes_ else "unknown"
                )

            # ëª¨ë“  ì»¬ëŸ¼ì´ ë²”ì£¼í˜•ì´ë¼ê³  ê°€ì •
            df[col] = df[col].astype(str)
            test = le.transform(df[col])
            df[col] = test

        logger.info("  ğŸª› Preprocessing Done.")
        
        return df
    


    def __feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        # TODO: Fill in if needed
        
        # df = sehoon.feat_elapsed(df)
        # df = sehoon.feat_elapsed_cumsum(df)
        # df = sehoon.feat_ass_correct_stats(df, 'mean')
        # df = sehoon.feat_user_answer_acc_per(df)
        # df = sehoon.feat_relative_answer_score(df)
        # df = sehoon.feat_normalized_elapsed(df)
        # df = sehoon.feat_relative_elapsed_time(df)
        
        logger.info("  ğŸ”¨ Feature Engineering Done.")
        
        return df

    # csv data ë¡œë“œ + feature engineering + ì •ë¦¬ -> íŠœí”Œë¡œ ë°˜í™˜
    def load_data_from_file(self, file_name: str, is_train: bool = True) -> np.ndarray:
        dtype = {'userID': 'int16', 
                 'answerCode': 'int8', 
                 'KnowledgeTag': 'int16'}
        csv_file_path = os.path.join(self.args.data_dir, file_name) 
        # dytypeí•˜ê³  parse_dates=['Timestamp']ì¶”ê°€í•˜ê³  ìœ„ì— convert_timeì£¼ì„
        df= pd.read_csv(csv_file_path, dtype=dtype, parse_dates=['Timestamp']) # , nrows=100000)
        df = self.__feature_engineering(df)
        df = self.__preprocessing(df, is_train)

        # [ê±´ìš°] categoryë³€ìˆ˜ì˜ uniqueì˜ ê°œìˆ˜ë¥¼ argsì— ë“±ë¡ -> ì¶”í›„ featureë¥¼ embeddingí•  ì‹œì— embedding_layerì˜ input í¬ê¸°ë¥¼ ê²°ì •í• ë•Œ ì‚¬ìš© 
        for col in self.args.cat_cols:
            setattr(self.args, f'n_{col}', len(np.load(os.path.join(self.args.asset_dir, f'{col}_classes.npy'))))
        
        self.args.columns = self.args.cat_cols + self.args.con_cols + self.args.tgt_col # [ê±´ìš°] ìë™í™” ì½”ë“œ(ì¶”ê°€)
        group = (
            df
            .groupby("userID")
            .apply(
                lambda r: tuple(r[col].values for col in self.args.columns) # [ê±´ìš°] ìë™í™” ì½”ë“œ(ì¶”ê°€)
                )
            )
        
        ################## ë°ì´í„° í™•ì¸ #####################
        get_data_info(logger, df, group, self.args)
        
        return group.values 

    # (csv data ë¡œë“œ + feature engineering + ì •ë¦¬)í•œ ê²ƒì„ noneì´ì—ˆë˜ self.train_dataì— ì •ì˜
    def load_train_data(self, file_name: str) -> None:
        self.train_data = self.load_data_from_file(file_name) 

    # (csv data ë¡œë“œ + feature engineering + ì •ë¦¬)í•œ ê²ƒì„ noneì´ì—ˆë˜ self.test_dataì— ì •ì˜
    def load_test_data(self, file_name: str) -> None:
        self.test_data = self.load_data_from_file(file_name, is_train=False) 


class DKTDataset(torch.utils.data.Dataset):
    def __init__(self, data: np.ndarray, args):
        self.data = data
        self.max_seq_len = args.max_seq_len
        self.args = args
        self.args.columns = self.args.columns + ['mask', 'interaction'] # columnsì— mask, interaction ì¶”ê°€

    # ì£¼ì–´ì§„ ì¸ë±ìŠ¤ index(row)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ
    def __getitem__(self, index: int) -> dict:
        row = self.data[index] 
        
        # [ê±´ìš°] ìë™í™” ì½”ë“œ(ì¶”ê°€)
        data={}
        columns = self.args.cat_cols + self.args.con_cols + self.args.tgt_col
        for i, col in enumerate(columns):
            if i < len(self.args.cat_cols): # categorical
                data[col] = torch.tensor(row[i] + 1, dtype=torch.int) # embedding ë•Œë¬¸ì— 0ê³¼ êµ¬ë¶„í•˜ë ¤ê³  +1í•¨
            else: # continous
                data[col] = torch.tensor(row[i], dtype=torch.float) 
        

        # Generate mask: max seq lenì„ ê³ ë ¤í•˜ì—¬ì„œ ì´ë³´ë‹¤ ê¸¸ë©´ ìë¥´ê³  ì•„ë‹ ê²½ìš° ê·¸ëŒ€ë¡œ ëƒ…ë‘”ë‹¤
        seq_len = len(row[0])
        if seq_len > self.max_seq_len:
            for k, seq in data.items():
                data[k] = seq[-self.max_seq_len:]
            mask = torch.ones(self.max_seq_len, dtype=torch.int16)
        else:
            for k, seq in data.items():
                # Pre-padding non-valid sequences
                tmp = torch.zeros(self.max_seq_len)
                tmp[self.max_seq_len-seq_len:] = data[k]
                data[k] = tmp
            mask = torch.zeros(self.max_seq_len, dtype=torch.int16)
            mask[-seq_len:] = 1
        data["mask"] = mask
        
        # Generate interaction(ìƒˆë¡œ ìƒì„±) : ì´ì „ sequenceë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ ì‘ì—…
        interaction = data["answerCode"] + 1  # íŒ¨ë”©ì„ ìœ„í•´ correctê°’ì— 1ì„ ë”í•´ì¤€ë‹¤.(íŒ¨ë”©ì´ 0ì´ê¸° ë•Œë¬¸)
        # roll(shifts=1): í…ì„œ(ë˜ëŠ” ë°°ì—´)ë¥¼ í•œ ì¹¸ì”© ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ ex) [a, b, c, d] -> [d, a, b, c]
        interaction = interaction.roll(shifts=1) 
        interaction_mask = data["mask"].roll(shifts=1) 
        interaction_mask[0] = 0 # ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì˜®ê²¨ì„œ ìƒí˜¸ì‘ìš© ê³„ì‚°í•˜ëŠ”ë° ì²« ë²ˆì§¸ëŠ” ì´ì „ sequenceê°€ ì—†ì–´ì„œ 0ë„£ìŒ
        interaction = (interaction * interaction_mask).to(torch.int64) # ìƒí˜¸ì‘ìš© ê³„ì‚°
        data["interaction"] = interaction
        data = {k: v.int() for k, v in data.items()}
        return data

    def __len__(self) -> int:
        return len(self.data)


def get_loaders(args, train: np.ndarray, valid: np.ndarray) -> Tuple[torch.utils.data.DataLoader]:
    pin_memory = False
    train_loader, valid_loader = None, None

    if train is not None:
        trainset = DKTDataset(train, args)
        # torch.utils.data.DataLoader : modelì— feed(batch, shuffle, cpuì™€ gpuë³€í™˜)
        train_loader = torch.utils.data.DataLoader(
            trainset,
            num_workers=args.num_workers,
            shuffle=True,
            batch_size=args.batch_size,
            pin_memory=pin_memory,
        )
    if valid is not None:
        valset = DKTDataset(valid, args)
        valid_loader = torch.utils.data.DataLoader(
            valset,
            num_workers=args.num_workers,
            shuffle=False,
            batch_size=args.batch_size,
            pin_memory=pin_memory,
        )

    return train_loader, valid_loader

def get_loaders_kfold(args, train, train_idx, valid_idx) -> Tuple[torch.utils.data.DataLoader]:
    pin_memory = False
    train_loader, valid_loader = None, None

    trainset = DKTDataset(train, args)
    # torch.utils.data.DataLoader : modelì— feed(batch, shuffle, cpuì™€ gpuë³€í™˜)
    train_loader = torch.utils.data.DataLoader(
        trainset,
        num_workers=args.num_workers,
        batch_size=args.batch_size,
        pin_memory=pin_memory,
        sampler=train_idx
    )
    valset = DKTDataset(train, args)
    valid_loader = torch.utils.data.DataLoader(
        valset,
        num_workers=args.num_workers,
        batch_size=args.batch_size,
        pin_memory=pin_memory,
        sampler=valid_idx
    )

    return train_loader, valid_loader

def split_data(data: np.ndarray,
               ratio: float = 0.7,
               shuffle: bool = True,
               seed: int = 0) -> Tuple[np.ndarray]:
    """
    split data into two parts with a given ratio.
    """
    if shuffle:
        random.seed(seed)  # fix to default seed 0
        random.shuffle(data)

    size = int(len(data) * ratio)
    data_1 = data[:size]
    data_2 = data[size:]
    
    logger.info(f"Split Data Info")
    logger.info(f"  Train : {len(data_1)}")
    logger.info(f"  Valid : {len(data_2)}")
    
    return data_1, data_2


################### data augmentation ###################
def data_augmentation(train_data, args):
    
    augmented_train_data = slidding_window(train_data, args)
    
    logger.info(f"  Data Augmentation applied. Train data {len(train_data)} -> {len(augmented_train_data)}")
    
    return augmented_train_data

def slidding_window(data, args):
    window_size = args.max_seq_len
    stride = args.max_seq_len
    
    augmented_datas_list = []
    for row in data:
        seq_len = len(row[0])

        # ë§Œì•½ window í¬ê¸°ë³´ë‹¤ seq lenì´ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ augmentationì„ í•˜ì§€ ì•ŠëŠ”ë‹¤
        if seq_len <= window_size:
            augmented_datas_list.append(row)
        else:
            total_window = ((seq_len - window_size) // stride) + 1

            # ì•ì—ì„œë¶€í„° slidding window ì ìš©
            for window_i in range(total_window):
                # windowë¡œ ì˜ë¦° ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” ë¦¬ìŠ¤íŠ¸
                window_data = []
                for col in row:
                    window_data.append(
                        col[window_i * stride : window_i * stride + window_size]
                    )

                # Shuffle
                # ë§ˆì§€ë§‰ ë°ì´í„°ì˜ ê²½ìš° shuffleì„ í•˜ì§€ ì•ŠëŠ”ë‹¤
                if args.shuffle and window_i + 1 != total_window:
                    shuffle_datas = shuffle(window_data, window_size, args)
                    augmented_datas_list += shuffle_datas
                else:
                    augmented_datas_list.append(tuple(window_data))

            # slidding windowì—ì„œ ë’·ë¶€ë¶„ì´ ëˆ„ë½ë  ê²½ìš° ì¶”ê°€
            total_len = window_size + (stride * (total_window - 1))
            if seq_len != total_len:
                window_data = []
                for col in row:
                    window_data.append(col[-window_size:])
                augmented_datas_list.append(tuple(window_data))

    return augmented_datas_list

def shuffle(data, data_size, args):
    shuffle_datas = []
    for i in range(args.shuffle_n):
        # shuffle íšŸìˆ˜ë§Œí¼ windowë¥¼ ëœë¤í•˜ê²Œ ê³„ì† ì„ì–´ì„œ ë°ì´í„°ë¡œ ì¶”ê°€
        shuffle_data = []
        random_index = np.random.permutation(data_size)
        for col in data:
            shuffle_data.append(col[random_index])
        shuffle_datas.append(tuple(shuffle_data))
    return shuffle_datas
################### data augmentation ###################